{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import webbrowser\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "\n",
    "from evolution.evaluation.evaluator import EnROADSEvaluator\n",
    "from evolution.utils import process_config\n",
    "from experiments.experiment_utils import NNExperimenter, DirectExperimenter\n",
    "from enroadspy import load_input_specs, id_to_name, name_to_id\n",
    "from enroadspy.generate_url import actions_to_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = Path(\"results/actions\")\n",
    "results_df = pd.read_csv(results_dir / \"results.csv\")\n",
    "n_generations = results_df[\"gen\"].max()\n",
    "with open(results_dir / \"config.yml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config = process_config(config)\n",
    "\n",
    "context = config[\"context\"]\n",
    "actions = config[\"actions\"]\n",
    "outcomes = config[\"outcomes\"]\n",
    "outcome_keys = list(outcomes.keys())\n",
    "n_elites = config[\"evolution_params\"][\"n_elites\"]\n",
    "print(n_generations, outcomes, len(actions), len(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_specs = load_input_specs()\n",
    "\n",
    "evaluator = EnROADSEvaluator(context, actions, outcomes, n_jobs=1, batch_size=config[\"batch_size\"], device=config[\"device\"])\n",
    "if len(context) > 0:\n",
    "    experimenter = NNExperimenter(results_dir)\n",
    "else:\n",
    "    experimenter = DirectExperimenter(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_actions_dict(actions_dict: dict):\n",
    "    input_specs = load_input_specs()\n",
    "    filtered = {}\n",
    "    for action in actions_dict:\n",
    "        default = input_specs[input_specs[\"id\"] == action][\"defaultValue\"].values[0]\n",
    "        if actions_dict[action] != default:\n",
    "            filtered[action] = actions_dict[action]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_actions_taken(gen: int, results_df: pd.DataFrame, magnitude=False):\n",
    "    gen_df = results_df[results_df[\"gen\"] == gen]\n",
    "    gen_df = gen_df[gen_df[\"rank\"] == 1]\n",
    "    pareto_len = len(gen_df)\n",
    "    actions_count = {}\n",
    "    for cand_id in gen_df[\"cand_id\"]:\n",
    "        [context_actions_dict], _, _ = experimenter.get_candidate_results(cand_id)\n",
    "        for action in context_actions_dict:\n",
    "            row = input_specs[input_specs[\"id\"] == action].iloc[0]\n",
    "            default_value = row[\"defaultValue\"]\n",
    "            if action not in context and context_actions_dict[action] != default_value:\n",
    "                if not magnitude:\n",
    "                    actions_count[action] = actions_count.get(action, 0) + 1\n",
    "                else:\n",
    "                    min_value = row[\"minValue\"] if row[\"kind\"] == \"slider\" else row[\"offValue\"]\n",
    "                    max_value = row[\"maxValue\"] if row[\"kind\"] == \"slider\" else row[\"onValue\"]\n",
    "                    if min_value == max_value:\n",
    "                        continue\n",
    "                    norm = (context_actions_dict[action] - min_value) / (max_value - min_value)\n",
    "                    default_norm = (default_value - min_value) / (max_value - min_value)\n",
    "                    actions_count[action] += abs(norm - default_norm)\n",
    "\n",
    "    # Switch to nice name, normalize values\n",
    "    actions_count = {action: count / pareto_len for action, count in actions_count.items()}\n",
    "    actions_count = dict(sorted(actions_count.items(), key=lambda item: item[1], reverse=True))\n",
    "    pretty_actions_count = {id_to_name(action, input_specs): count for action, count in actions_count.items()}\n",
    "\n",
    "    print(pretty_actions_count)\n",
    "    fig, ax = plt.subplots(figsize=(5, 20))\n",
    "    ax.barh(pretty_actions_count.keys(), pretty_actions_count.values())\n",
    "    ax.set_title(\"Proportion of Candidates in Pareto with Action Taken\")\n",
    "    ax.grid()\n",
    "    ax.invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "    return actions_count\n",
    "\n",
    "actions_count = plot_actions_taken(n_generations, results_df, magnitude=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pareto(outcome1: str, outcome2: str, gen: int, results_df: pd.DataFrame, colored_ids=[]):\n",
    "\n",
    "    gen_df = results_df[(results_df[\"gen\"] == gen) & (results_df[\"rank\"] == 1)]\n",
    "\n",
    "    plt.scatter(gen_df[outcome1], gen_df[outcome2])\n",
    "\n",
    "    colors = [\"pink\", \"lightgreen\"]\n",
    "    for colored_id, color in zip(colored_ids, colors):\n",
    "        colored_df = gen_df[gen_df[\"cand_id\"] == colored_id]\n",
    "        plt.scatter(colored_df[outcome1], colored_df[outcome2], color=color)\n",
    "\n",
    "    title = f\"{outcome1} vs {outcome2} Final Pareto\"\n",
    "    plt.title(title)\n",
    "    plt.ylabel(outcome2)\n",
    "    plt.xlabel(outcome1)\n",
    "    plt.show()\n",
    "\n",
    "plot_pareto(outcome_keys[0], outcome_keys[2], n_generations, results_df)\n",
    "plot_pareto(outcome_keys[1], outcome_keys[2], n_generations, results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_special_cands(outcome: str, results_df: pd.DataFrame, gen: int):\n",
    "    \"\"\"\n",
    "    Sort by largest difference in outcome. Then get the 2 candidates with the largest difference.\n",
    "    \"\"\"\n",
    "    pareto_df = results_df[(results_df[\"gen\"] == gen) & (results_df[\"rank\"] == 1)]\n",
    "    pareto_df = pareto_df.sort_values(outcome, ascending=False)\n",
    "    diff = pareto_df[outcome].diff()\n",
    "    diff = diff.fillna(0)\n",
    "\n",
    "    # Get the index of the row where the largest drop occurs (this is the lower value of the pair)\n",
    "    top_idx = diff.idxmax()\n",
    "\n",
    "    # Get the position (iloc) of that row to access the previous row\n",
    "    top_pos = pareto_df.index.get_loc(top_idx)\n",
    "\n",
    "    # if lower_pos == 0:\n",
    "    #     raise ValueError(\"Largest diff is at the top row; cannot get the row above it.\")\n",
    "\n",
    "    bot_pos = top_pos + 1\n",
    "    bot_idx = pareto_df.index[bot_pos]\n",
    "\n",
    "    top = pareto_df.loc[top_idx]\n",
    "    bot = pareto_df.loc[bot_idx]\n",
    "\n",
    "    return top[\"cand_id\"], bot[\"cand_id\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special = get_special_cands(outcome_keys[2], results_df, n_generations)\n",
    "print(special)\n",
    "plot_pareto(outcome_keys[0], outcome_keys[2], n_generations, results_df, special)\n",
    "plot_pareto(outcome_keys[1], outcome_keys[2], n_generations, results_df, special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_actions(cand_1: str, cand_2: str):\n",
    "    [ca_1], _, _ = experimenter.get_candidate_results(cand_1)\n",
    "    [ca_2], _, _ = experimenter.get_candidate_results(cand_2)\n",
    "\n",
    "    total_actions = list(set(ca_1.keys()).union(set(ca_2.keys())))\n",
    "\n",
    "    all_actions = []\n",
    "    norm1s = []\n",
    "    norm2s = []\n",
    "    defaults = []\n",
    "    mins = []\n",
    "    maxes = []\n",
    "\n",
    "    for action in total_actions:\n",
    "        row = input_specs[input_specs[\"id\"] == action].iloc[0]\n",
    "        default_value = row[\"defaultValue\"]\n",
    "        min_value = row[\"minValue\"] if row[\"kind\"] == \"slider\" else row[\"offValue\"]\n",
    "        max_value = row[\"maxValue\"] if row[\"kind\"] == \"slider\" else row[\"onValue\"]\n",
    "\n",
    "        if ca_1[action] == default_value and ca_2[action] == default_value:\n",
    "            continue\n",
    "\n",
    "        if min_value == max_value:\n",
    "            norm_1 = 1\n",
    "            norm_2 = 1\n",
    "            norm_default = 1\n",
    "\n",
    "        else:\n",
    "            norm_default = (default_value - min_value) / (max_value - min_value)\n",
    "            norm_1 = (ca_1[action] - min_value) / (max_value - min_value)\n",
    "            norm_2 = (ca_2[action] - min_value) / (max_value - min_value)\n",
    "\n",
    "            diff_1 = norm_1 - norm_default\n",
    "            diff_2 = norm_2 - norm_default\n",
    "\n",
    "        norm1s.append(diff_1)\n",
    "        norm2s.append(diff_2)\n",
    "        defaults.append(norm_default)\n",
    "        mins.append(min_value)\n",
    "        maxes.append(max_value)\n",
    "        all_actions.append(action)\n",
    "\n",
    "    df = pd.DataFrame({\"action\": all_actions,\n",
    "                       \"norm1\": norm1s,\n",
    "                       \"norm2\": norm2s,\n",
    "                       \"default\": defaults,\n",
    "                       \"min\": mins,\n",
    "                       \"max\": maxes})\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.scatter(df[\"norm1\"], range(len(df)), label=cand_1, color=\"pink\")\n",
    "    ax.scatter(df[\"norm2\"], range(len(df)), label=cand_2, color=\"lightgreen\")\n",
    "    ax.axvline(x=0, color=\"black\", linestyle=\"--\", label=\"default\")\n",
    "\n",
    "    ax.set_xlabel(\"Normalized Difference from Default\")\n",
    "    ax.set_yticks(range(len(df)), df[\"action\"].apply(lambda x: id_to_name(x, input_specs)))\n",
    "    ax.set_title(\"Actions Comparison\")\n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "compare_actions(*special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_actions_dict(cand_id):\n",
    "    [context_actions_dict], [outcomes_df], _ = experimenter.get_candidate_results(cand_id)\n",
    "    true_ca_dict = {}\n",
    "    for action in context_actions_dict:\n",
    "        default_value = input_specs[input_specs[\"id\"] == action][\"defaultValue\"].values[0]\n",
    "        if context_actions_dict[action] != default_value:\n",
    "            true_ca_dict[action] = context_actions_dict[action]\n",
    "    print(true_ca_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cand_id in special:\n",
    "    print_actions_dict(cand_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_pareto_by_action(action: str, outcome_1: str, outcome_2: str, gen: int, results_df: pd.DataFrame):\n",
    "    pareto_df = results_df[(results_df[\"gen\"] == gen) & (results_df[\"rank\"] == 1)]\n",
    "\n",
    "    action = name_to_id(action, input_specs)\n",
    "\n",
    "    action_vals = []\n",
    "    for cand_id in pareto_df[\"cand_id\"]:\n",
    "        [context_actions_dict], _, _ = experimenter.get_candidate_results(cand_id)\n",
    "        action_vals.append(context_actions_dict[action])\n",
    "    \n",
    "    pareto_df[\"action\"] = action_vals\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    min_val = min(action_vals)\n",
    "    max_val = max(action_vals)\n",
    "    if min_val < 0 and max_val > 0:\n",
    "        cmap_params = {\"cmap\": \"PiYG\", \"vmin\": min(min_val, -max_val), \"vmax\": max(-min_val, max_val)}\n",
    "    else:\n",
    "        cmap_params = {\"cmap\": \"magma\"}\n",
    "    scatter = ax.scatter(pareto_df[outcome_1], pareto_df[outcome_2], c=pareto_df[\"action\"], **cmap_params)\n",
    "    ax.set_xlabel(outcome_1)\n",
    "    ax.set_ylabel(outcome_2)\n",
    "    cbar = fig.colorbar(scatter)\n",
    "    cbar.set_label(id_to_name(action, load_input_specs()), rotation=270, labelpad=15)\n",
    "    plt.show()\n",
    "\n",
    "color_pareto_by_action(\"Carbon tax initial target\", outcome_keys[0], outcome_keys[2], n_generations, results_df)\n",
    "color_pareto_by_action(\"Source tax oil boe\", outcome_keys[0], outcome_keys[2], n_generations, results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_df = results_df[(results_df[\"gen\"] == n_generations) & (results_df[\"rank\"] == 1)]\n",
    "[context_actions_dict], _, _ = experimenter.get_candidate_results(pareto_df.sort_values(outcome_keys[2])[\"cand_id\"].values[0])\n",
    "print(filter_actions_dict(context_actions_dict))\n",
    "if config[\"decomplexify\"]:\n",
    "    context_actions_dict = evaluator.decomplexify_actions_dict(context_actions_dict)\n",
    "url = actions_to_url(context_actions_dict)\n",
    "webbrowser.open(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_pareto_by_action(\"Carbon tax final target\", outcome_keys[0], outcome_keys[2], n_generations, results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enroads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
